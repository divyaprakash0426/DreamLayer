PRD: Luma Photon depth2img Node
1. Feature Overview
The goal is to create a new ComfyUI custom node named luma_photon_depth2img. This node will take an input image, generate a depth map using the local MiDaS model, and then call the external Luma Photon API to generate a new, "novel-view" image. The generated depth map will be saved for user reference.

2. Core Requirements & Deliverables
Based on the challenge description, the feature is complete when the following deliverables are met:

ComfyUI Node: A new node, luma_photon_depth2img, is available in the ComfyUI editor.

Input/Output:

Accepts an IMAGE input, making it chainable from a Load Image node.

Outputs the final IMAGE generated by the Luma Photon API.

Depth Estimation:

Locally runs the MiDaS model to automatically estimate a depth map from the input image.

Saves the generated MiDaS depth map to the outputs/depth/ directory.

API Integration:

Calls the Luma Photon API to generate the final image.

The input image should be used as a reference for the Photon API call.

Documentation & Configuration:

An inline docstring within the node's Python code must explain how to disable the depth estimation step for debugging purposes.

The feature must support CI/CD by ensuring the MiDaS model is downloaded during the GitHub Action's cache-restore step.

3. Technical Implementation Plan
This plan breaks down the development process into manageable steps, referencing the provided project documentation.

Step 1: Scaffolding the Custom Node
Create Node Directory: Inside ComfyUI/custom_nodes/, create a new directory, e.g., luma_photon_node.

Create Python File: Inside the new directory, create luma_photon_node.py. This will contain the main node logic.

Create __init__.py: In the same directory, create an __init__.py file to register your new node with ComfyUI's mapping system. This is a mandatory step for the node to be recognized.

Python

# __init__.py
from .luma_photon_node import LumaPhotonDepth2Img

NODE_CLASS_MAPPINGS = {
    "LumaPhotonDepth2Img": LumaPhotonDepth2Img
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "LumaPhotonDepth2Img": "Luma Photon (Depth)"
}

__all__ = ['NODE_CLASS_MAPPINGS', 'NODE_DISPLAY_NAME_MAPPINGS']
Step 2: Defining the Node Class
In luma_photon_node.py, define the LumaPhotonDepth2Img class. The structure should follow the custom node patterns found in the ComfyUI documentation.

Python

# luma_photon_node.py
import torch
from server import PromptServer # Needed to send data to frontend if required

class LumaPhotonDepth2Img:
    @classmethod
    def INPUT_TYPES(s):
        """Defines the input types for the node."""
        return {
            "required": {
                "image": ("IMAGE",),
                "prompt": ("STRING", {"multiline": True, "default": "A beautiful, photorealistic image"}),
                "api_key": ("STRING", {"multiline": False}),
                "disable_depth": ("BOOLEAN", {"default": False}),
            }
        }

    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "generate_novel_view"
    CATEGORY = "DreamLayer/API" # Or any other category you prefer

    def generate_novel_view(self, image, prompt, api_key, disable_depth):
        """The main execution function for the node."""
        # Main logic will go here
        pass
Step 3: Implementing MiDaS Depth Estimation
Add Dependency: Add timm and opencv-python to a requirements.txt file inside your luma_photon_node directory. The CI will use this to install dependencies.

MiDaS Logic: Implement the depth estimation logic within your generate_novel_view function.

Python

# Inside generate_novel_view method
if not disable_depth:
    # 1. Load MiDaS model from PyTorch Hub
    model_type = "MiDaS_small" # Use a smaller model for efficiency
    midas = torch.hub.load("intel-isl/MiDaS", model_type)
    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    midas.to(device)
    midas.eval()

    # 2. Load transforms
    midas_transforms = torch.hub.load("intel-isl/MiDaS", "transforms")
    transform = midas_transforms.small_transform

    # 3. Process input image tensor and generate depth map
    # (You'll need to convert the ComfyUI image tensor to a NumPy array for processing)
    # ... processing logic ...
    with torch.no_grad():
        prediction = midas(input_batch)
        prediction = torch.nn.functional.interpolate(...) # Resize to original

    # 4. Save the depth map
    # (Ensure the outputs/depth directory exists)
    depth_map_numpy = prediction.cpu().numpy()
    # ... logic to save numpy array as a PNG image to 'outputs/depth/' ...
Step 4: Integrating the Luma Photon API
Add Dependency: Add the luma-ai library to your requirements.txt.

API Call Logic: Use the provided API key to call the Luma Photon API.

Python

# Inside generate_novel_view method, after depth estimation
from lumaai import AsyncLumaAI

# 1. Initialize the client
# The API is async, you might need to handle this appropriately in the ComfyUI context
client = AsyncLumaAI(auth_token=api_key)

# 2. Upload input image to a temporary URL if not already a URL
# Luma's API expects a URL for the reference image. You may need to
# temporarily host the input image or use a service to get a public URL.
# The existing DreamLayer backend might have a way to serve images.
# Reference: POST /api/upload-controlnet-image

# 3. Call the Luma API
# generation = await client.generations.image.create(
#     prompt=prompt,
#     image_reference_url="URL_TO_YOUR_INPUT_IMAGE",
#     # model="photon-1", # Specify model if needed
# )

# 4. Poll for results
# The Luma API is asynchronous. You will need to poll the generation endpoint
# using the ID from the creation response until the image is ready.

# 5. Process the final image
# Once the result is available, download the image and convert it back
# into a ComfyUI-compatible tensor to return from the node.
Step 5: Finalizing the Node
Docstring: Add a detailed docstring to the generate_novel_view function explaining its purpose, parameters, and how to use the disable_depth flag for debugging.

Error Handling: Implement try...except blocks to gracefully handle potential errors during MiDaS model loading, depth estimation, or API calls.

CI/CD: To handle the model download in CI, you can leverage PyTorch Hub's caching. Ensure your GitHub Actions workflow for CI has caching enabled for the ~/.cache/torch/hub directory.

4. User Experience
The user will connect a Load Image node to the image input of the new Luma Photon (Depth) node.

They will fill in a text prompt and their api_key.

Upon queuing the prompt, the node will first process the image with MiDaS (a progress bar might be visible in the console), save the depth map, and then make the call to the Luma API.

The final, stylized image from Luma will be returned and can be connected to a Save Image or Preview Image node.


